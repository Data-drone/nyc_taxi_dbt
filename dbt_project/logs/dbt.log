2021-06-29 13:43:27.961886 (MainThread): Running with dbt=0.19.1
2021-06-29 13:43:27.981217 (MainThread): the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
2021-06-29 13:43:28.036416 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, defer=None, exclude=None, fail_fast=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/root/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2021-06-29 13:43:28.037142 (MainThread): Tracking: tracking
2021-06-29 13:43:28.038992 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9512c99d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9512ad96a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9512ad97c0>]}
2021-06-29 13:43:28.046633 (MainThread): Partial parsing not enabled
2021-06-29 13:43:28.047361 (MainThread): Parsing macros/adapters.sql
2021-06-29 13:43:28.067091 (MainThread): Parsing macros/materializations/table.sql
2021-06-29 13:43:28.069621 (MainThread): Parsing macros/materializations/snapshot.sql
2021-06-29 13:43:28.090050 (MainThread): Parsing macros/materializations/view.sql
2021-06-29 13:43:28.090555 (MainThread): Parsing macros/materializations/seed.sql
2021-06-29 13:43:28.102404 (MainThread): Parsing macros/materializations/incremental/strategies.sql
2021-06-29 13:43:28.107210 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-06-29 13:43:28.111835 (MainThread): Parsing macros/materializations/incremental/validate.sql
2021-06-29 13:43:28.118223 (MainThread): Parsing macros/core.sql
2021-06-29 13:43:28.121578 (MainThread): Parsing macros/adapters/common.sql
2021-06-29 13:43:28.156954 (MainThread): Parsing macros/schema_tests/unique.sql
2021-06-29 13:43:28.158397 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-06-29 13:43:28.160681 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-06-29 13:43:28.161957 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-06-29 13:43:28.163514 (MainThread): Parsing macros/etc/datetime.sql
2021-06-29 13:43:28.171011 (MainThread): Parsing macros/etc/query.sql
2021-06-29 13:43:28.171870 (MainThread): Parsing macros/etc/is_incremental.sql
2021-06-29 13:43:28.173242 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-06-29 13:43:28.174614 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-06-29 13:43:28.175366 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-06-29 13:43:28.176992 (MainThread): Parsing macros/materializations/helpers.sql
2021-06-29 13:43:28.184372 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-06-29 13:43:28.199742 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-06-29 13:43:28.224970 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-06-29 13:43:28.226442 (MainThread): Parsing macros/materializations/table/table.sql
2021-06-29 13:43:28.232079 (MainThread): Parsing macros/materializations/common/merge.sql
2021-06-29 13:43:28.243728 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-06-29 13:43:28.245280 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-06-29 13:43:28.276027 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-06-29 13:43:28.293521 (MainThread): Parsing macros/materializations/view/view.sql
2021-06-29 13:43:28.298883 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-06-29 13:43:28.307212 (MainThread): Partial parsing not enabled
2021-06-29 13:43:28.327400 (MainThread): Acquiring new spark connection "model.nyc_taxi.daily_trips".
2021-06-29 13:43:28.388641 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ca8fc587-5b59-4a18-8f31-e402b220ecdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9512839970>]}
2021-06-29 13:43:28.393764 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ca8fc587-5b59-4a18-8f31-e402b220ecdc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f95128a1bb0>]}
2021-06-29 13:43:28.393913 (MainThread): Found 1 model, 3 tests, 0 snapshots, 0 analyses, 158 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-06-29 13:43:28.394430 (MainThread): 
2021-06-29 13:43:28.394639 (MainThread): Acquiring new spark connection "master".
2021-06-29 13:43:28.395659 (ThreadPoolExecutor-1_0): Acquiring new spark connection "list_None_nyc_taxi_analytics".
2021-06-29 13:43:28.405633 (ThreadPoolExecutor-1_0): NotImplemented: add_begin_query
2021-06-29 13:43:28.405765 (ThreadPoolExecutor-1_0): Using spark connection "list_None_nyc_taxi_analytics".
2021-06-29 13:43:28.405822 (ThreadPoolExecutor-1_0): On list_None_nyc_taxi_analytics: /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "list_None_nyc_taxi_analytics"} */
show table extended in nyc_taxi_analytics like '*'
  
2021-06-29 13:43:28.405896 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2021-06-29 13:43:28.422192 (ThreadPoolExecutor-1_0): TOpenSessionResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), serverProtocolVersion=5, sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\x0e0\xc6!\x9a`CH\x99\x86#m\xef5\x7f2', secret=b'&\x9b\xf9q\x93\x8dBN\xa4CC\xe6var(')), configuration={})
2021-06-29 13:43:28.422704 (ThreadPoolExecutor-1_0): USE `default`
2021-06-29 13:43:28.422999 (ThreadPoolExecutor-1_0): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\x0e0\xc6!\x9a`CH\x99\x86#m\xef5\x7f2', secret=b'&\x9b\xf9q\x93\x8dBN\xa4CC\xe6var(')), statement='USE `default`', confOverlay=None, runAsync=False, queryTimeout=0)
2021-06-29 13:43:28.448059 (ThreadPoolExecutor-1_0): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\x99\xb9+2\xc9\xba@e\xa9\x05hI\nG\x0b\xa4', secret=b'1"\r\xa3\x81XC6\x91\xf3\x96>\x06\x0fZ\x97'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:43:28.448959 (ThreadPoolExecutor-1_0): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:43:28.449112 (ThreadPoolExecutor-1_0): /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "list_None_nyc_taxi_analytics"} */
show table extended in nyc_taxi_analytics like '*'
  
2021-06-29 13:43:28.449203 (ThreadPoolExecutor-1_0): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\x0e0\xc6!\x9a`CH\x99\x86#m\xef5\x7f2', secret=b'&\x9b\xf9q\x93\x8dBN\xa4CC\xe6var(')), statement='/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "list_None_nyc_taxi_analytics"} */\nshow table extended in nyc_taxi_analytics like \'*\'\n  ', confOverlay=None, runAsync=True, queryTimeout=0)
2021-06-29 13:43:28.451408 (ThreadPoolExecutor-1_0): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\xbe\xc5\x97\x87o\xd6G\x0e\xaf\xb6\xe6a\xc3e\x1c"', secret=b"\xc8\x8d\xdd\xd9n\x9fM\xba\xa6\x86\xfb\xdf\xad\xd6G'"), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:43:28.503809 (ThreadPoolExecutor-1_0): TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'nyc_taxi_analytics' not found\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'nyc_taxi_analytics' not found\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:192)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:951)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:932)\n\tat org.apache.spark.sql.execution.command.ShowTablesCommand.$anonfun$run$42(tables.scala:868)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.execution.command.ShowTablesCommand.run(tables.scala:868)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:43:28.504097 (ThreadPoolExecutor-1_0): Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'nyc_taxi_analytics' not found\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'nyc_taxi_analytics' not found\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:192)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:951)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:932)\n\tat org.apache.spark.sql.execution.command.ShowTablesCommand.$anonfun$run$42(tables.scala:868)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.execution.command.ShowTablesCommand.run(tables.scala:868)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:43:28.504190 (ThreadPoolExecutor-1_0): Poll status: 5
2021-06-29 13:43:28.504276 (ThreadPoolExecutor-1_0): Error while running:
/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "list_None_nyc_taxi_analytics"} */
show table extended in nyc_taxi_analytics like '*'
  
2021-06-29 13:43:28.504338 (ThreadPoolExecutor-1_0): Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'nyc_taxi_analytics' not found
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'nyc_taxi_analytics' not found
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:192)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:951)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:932)
  	at org.apache.spark.sql.execution.command.ShowTablesCommand.$anonfun$run$42(tables.scala:868)
  	at scala.Option.map(Option.scala:230)
  	at org.apache.spark.sql.execution.command.ShowTablesCommand.run(tables.scala:868)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	... 16 more
  
2021-06-29 13:43:28.504528 (ThreadPoolExecutor-1_0): Error while running:
macro list_relations_without_caching
2021-06-29 13:43:28.504588 (ThreadPoolExecutor-1_0): Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'nyc_taxi_analytics' not found
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:748)
    Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'nyc_taxi_analytics' not found
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:192)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:951)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:932)
    	at org.apache.spark.sql.execution.command.ShowTablesCommand.$anonfun$run$42(tables.scala:868)
    	at scala.Option.map(Option.scala:230)
    	at org.apache.spark.sql.execution.command.ShowTablesCommand.run(tables.scala:868)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
    	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
    	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
    	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	... 16 more
    
2021-06-29 13:43:28.504817 (ThreadPoolExecutor-1_0): On list_None_nyc_taxi_analytics: ROLLBACK
2021-06-29 13:43:28.504885 (ThreadPoolExecutor-1_0): NotImplemented: rollback
2021-06-29 13:43:28.504944 (ThreadPoolExecutor-1_0): On list_None_nyc_taxi_analytics: Close
2021-06-29 13:43:28.505535 (ThreadPoolExecutor-1_0): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:43:28.506187 (MainThread): 13:43:28 | Concurrency: 1 threads (target='dev')
2021-06-29 13:43:28.506313 (MainThread): 13:43:28 | 
2021-06-29 13:43:28.512414 (Thread-1): Began running node test.nyc_taxi.not_null_daily_trips_date
2021-06-29 13:43:28.512666 (Thread-1): 13:43:28 | 1 of 3 START test not_null_daily_trips_date.......................... [RUN]
2021-06-29 13:43:28.513075 (Thread-1): Acquiring new spark connection "test.nyc_taxi.not_null_daily_trips_date".
2021-06-29 13:43:28.513286 (Thread-1): unclosed <socket.socket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.64.25', 51346), raddr=('192.168.64.6', 10000)>
2021-06-29 13:43:28.513445 (Thread-1): Compiling test.nyc_taxi.not_null_daily_trips_date
2021-06-29 13:43:28.520991 (Thread-1): Writing injected SQL for node "test.nyc_taxi.not_null_daily_trips_date"
2021-06-29 13:43:28.521337 (Thread-1): finished collecting timing info
2021-06-29 13:43:28.521468 (Thread-1): NotImplemented: add_begin_query
2021-06-29 13:43:28.521529 (Thread-1): Using spark connection "test.nyc_taxi.not_null_daily_trips_date".
2021-06-29 13:43:28.521585 (Thread-1): On test.nyc_taxi.not_null_daily_trips_date: /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "node_id": "test.nyc_taxi.not_null_daily_trips_date"} */

    
    



select count(*) as validation_errors
from nyc_taxi_analytics.daily_trips
where date is null



2021-06-29 13:43:28.521660 (Thread-1): Opening a new connection, currently in state closed
2021-06-29 13:43:28.533808 (Thread-1): TOpenSessionResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), serverProtocolVersion=5, sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'YD\x1aB\x00kH\xef\xb6,\xf1\xa9\xceV\xeb;', secret=b'\\\x0f\xa1\x0f\xc2^@\x8a\x95=\x85\xc2\x99\xa5\x87\xee')), configuration={})
2021-06-29 13:43:28.534078 (Thread-1): USE `default`
2021-06-29 13:43:28.534231 (Thread-1): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'YD\x1aB\x00kH\xef\xb6,\xf1\xa9\xceV\xeb;', secret=b'\\\x0f\xa1\x0f\xc2^@\x8a\x95=\x85\xc2\x99\xa5\x87\xee')), statement='USE `default`', confOverlay=None, runAsync=False, queryTimeout=0)
2021-06-29 13:43:28.550871 (Thread-1): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'k\x8a\x86\xeb\x12\x01A\x1c\x9b\xd7\xecN5\xf8&s', secret=b'Y\xc2\x04\xda\xd5\x81E\xe0\xa4&\xf0\x8dL\x10\xd31'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:43:28.551615 (Thread-1): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:43:28.551791 (Thread-1): /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "node_id": "test.nyc_taxi.not_null_daily_trips_date"} */

    
    



select count(*) as validation_errors
from nyc_taxi_analytics.daily_trips
where date is null



2021-06-29 13:43:28.551885 (Thread-1): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'YD\x1aB\x00kH\xef\xb6,\xf1\xa9\xceV\xeb;', secret=b'\\\x0f\xa1\x0f\xc2^@\x8a\x95=\x85\xc2\x99\xa5\x87\xee')), statement='/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "node_id": "test.nyc_taxi.not_null_daily_trips_date"} */\n\n    \n    \n\n\n\nselect count(*) as validation_errors\nfrom nyc_taxi_analytics.daily_trips\nwhere date is null\n\n\n', confOverlay=None, runAsync=True, queryTimeout=0)
2021-06-29 13:43:28.553454 (Thread-1): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\x85\x00\xc6\xdd\xa0{H5\xbcO\xab,\x81\xfe>\xb5', secret=b'\xd0\xc3\xa6h;\x0eJ\xfc\x89[0"\x16q\xb9E'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:43:28.597102 (Thread-1): TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\t... 16 more\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.sources.DeltaSourceUtils$\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 91 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:43:28.597446 (Thread-1): Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\t... 16 more\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.sources.DeltaSourceUtils$\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 91 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:43:28.597566 (Thread-1): Poll status: 5
2021-06-29 13:43:28.597647 (Thread-1): Error while running:
/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "node_id": "test.nyc_taxi.not_null_daily_trips_date"} */

    
    



select count(*) as validation_errors
from nyc_taxi_analytics.daily_trips
where date is null



2021-06-29 13:43:28.597710 (Thread-1): Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
  	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
  	at scala.Option.orElse(Option.scala:447)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  	at scala.collection.immutable.List.foldLeft(List.scala:89)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
  	at scala.collection.immutable.List.foreach(List.scala:392)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	... 16 more
  Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.sources.DeltaSourceUtils$
  	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
  	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
  	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
  	... 91 more
  
2021-06-29 13:43:28.597932 (Thread-1): finished collecting timing info
2021-06-29 13:43:28.598051 (Thread-1): On test.nyc_taxi.not_null_daily_trips_date: ROLLBACK
2021-06-29 13:43:28.598113 (Thread-1): NotImplemented: rollback
2021-06-29 13:43:28.598168 (Thread-1): On test.nyc_taxi.not_null_daily_trips_date: Close
2021-06-29 13:43:28.598915 (Thread-1): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:43:28.599270 (Thread-1): Runtime Error in test not_null_daily_trips_date (models/schema.yml)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:748)
    Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
    	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
    	at scala.Option.orElse(Option.scala:447)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
    	at scala.collection.immutable.List.foreach(List.scala:392)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	... 16 more
    Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.sources.DeltaSourceUtils$
    	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
    	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
    	... 91 more
    
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 263, in exception_handler
    yield
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 80, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 208, in execute
    dbt.exceptions.raise_database_error(poll_state.errorMessage)
  File "/usr/local/lib/python3.8/site-packages/dbt/exceptions.py", line 439, in raise_database_error
    raise DatabaseException(msg, node)
dbt.exceptions.DatabaseException: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
  	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
  	at scala.Option.orElse(Option.scala:447)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  	at scala.collection.immutable.List.foldLeft(List.scala:89)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
  	at scala.collection.immutable.List.foreach(List.scala:392)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	... 16 more
  Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.sources.DeltaSourceUtils$
  	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
  	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
  	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
  	... 91 more
  

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/task/base.py", line 344, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/base.py", line 287, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/base.py", line 389, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/test.py", line 85, in execute
    failed_rows = self.execute_schema_test(test)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/test.py", line 62, in execute_schema_test
    res, table = self.adapter.execute(
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 227, in execute
    return self.connections.execute(
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 124, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 87, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 276, in exception_handler
    raise dbt.exceptions.RuntimeException(str(exc))
dbt.exceptions.RuntimeException: Runtime Error in test not_null_daily_trips_date (models/schema.yml)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:748)
    Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
    	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
    	at scala.Option.orElse(Option.scala:447)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
    	at scala.collection.immutable.List.foreach(List.scala:392)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	... 16 more
    Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.sources.DeltaSourceUtils$
    	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
    	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
    	... 91 more
2021-06-29 13:43:28.600597 (Thread-1): 13:43:28 | 1 of 3 ERROR not_null_daily_trips_date............................... [ERROR in 0.09s]
2021-06-29 13:43:28.600727 (Thread-1): Finished running node test.nyc_taxi.not_null_daily_trips_date
2021-06-29 13:43:28.600845 (Thread-1): Began running node test.nyc_taxi.not_null_daily_trips_num_trips
2021-06-29 13:43:28.601073 (Thread-1): 13:43:28 | 2 of 3 START test not_null_daily_trips_num_trips..................... [RUN]
2021-06-29 13:43:28.601319 (Thread-1): Acquiring new spark connection "test.nyc_taxi.not_null_daily_trips_num_trips".
2021-06-29 13:43:28.601436 (Thread-1): unclosed <socket.socket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.64.25', 51352), raddr=('192.168.64.6', 10000)>
2021-06-29 13:43:28.601546 (Thread-1): Compiling test.nyc_taxi.not_null_daily_trips_num_trips
2021-06-29 13:43:28.605816 (Thread-1): Writing injected SQL for node "test.nyc_taxi.not_null_daily_trips_num_trips"
2021-06-29 13:43:28.606085 (Thread-1): finished collecting timing info
2021-06-29 13:43:28.606230 (Thread-1): NotImplemented: add_begin_query
2021-06-29 13:43:28.606296 (Thread-1): Using spark connection "test.nyc_taxi.not_null_daily_trips_num_trips".
2021-06-29 13:43:28.606361 (Thread-1): On test.nyc_taxi.not_null_daily_trips_num_trips: /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "node_id": "test.nyc_taxi.not_null_daily_trips_num_trips"} */

    
    



select count(*) as validation_errors
from nyc_taxi_analytics.daily_trips
where num_trips is null



2021-06-29 13:43:28.606434 (Thread-1): Opening a new connection, currently in state closed
2021-06-29 13:43:28.619269 (Thread-1): TOpenSessionResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), serverProtocolVersion=5, sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\x17X)9I\x19KE\xa8M\x8b\xe1/\xf3\xb7\xc4', secret=b'\xcc\xcc\x9d\x12\xe0lB\xad\x981>!\xf85\xf6&')), configuration={})
2021-06-29 13:43:28.619566 (Thread-1): USE `default`
2021-06-29 13:43:28.619678 (Thread-1): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\x17X)9I\x19KE\xa8M\x8b\xe1/\xf3\xb7\xc4', secret=b'\xcc\xcc\x9d\x12\xe0lB\xad\x981>!\xf85\xf6&')), statement='USE `default`', confOverlay=None, runAsync=False, queryTimeout=0)
2021-06-29 13:43:28.636007 (Thread-1): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'w\\\xf8\xe8\xcb\x8eE\x11\xad\xaf\xf3\xe4\xb2\x183\x1d', secret=b'(\x82d[MhH\x15\xa7\xe2\xfa\xe3\\\xc5y\xc9'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:43:28.637090 (Thread-1): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:43:28.637327 (Thread-1): /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "node_id": "test.nyc_taxi.not_null_daily_trips_num_trips"} */

    
    



select count(*) as validation_errors
from nyc_taxi_analytics.daily_trips
where num_trips is null



2021-06-29 13:43:28.637481 (Thread-1): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\x17X)9I\x19KE\xa8M\x8b\xe1/\xf3\xb7\xc4', secret=b'\xcc\xcc\x9d\x12\xe0lB\xad\x981>!\xf85\xf6&')), statement='/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "node_id": "test.nyc_taxi.not_null_daily_trips_num_trips"} */\n\n    \n    \n\n\n\nselect count(*) as validation_errors\nfrom nyc_taxi_analytics.daily_trips\nwhere num_trips is null\n\n\n', confOverlay=None, runAsync=True, queryTimeout=0)
2021-06-29 13:43:28.639492 (Thread-1): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\xc5\x17m{\xa3\x10I\xa4\xb9g)\xe9\x82w\xe9\xd4', secret=b'\xae\x16\xb6\xa9\xe8\xa0B\x1f\xbf\n\x87\x14*\x05}:'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:43:28.646370 (Thread-1): TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:43:28.646596 (Thread-1): Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:43:28.646710 (Thread-1): Poll status: 5
2021-06-29 13:43:28.646792 (Thread-1): Error while running:
/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "node_id": "test.nyc_taxi.not_null_daily_trips_num_trips"} */

    
    



select count(*) as validation_errors
from nyc_taxi_analytics.daily_trips
where num_trips is null



2021-06-29 13:43:28.646854 (Thread-1): Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
  	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
  	at scala.Option.orElse(Option.scala:447)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  	at scala.collection.immutable.List.foldLeft(List.scala:89)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
  	at scala.collection.immutable.List.foreach(List.scala:392)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	... 16 more
  
2021-06-29 13:43:28.647071 (Thread-1): finished collecting timing info
2021-06-29 13:43:28.647207 (Thread-1): On test.nyc_taxi.not_null_daily_trips_num_trips: ROLLBACK
2021-06-29 13:43:28.647273 (Thread-1): NotImplemented: rollback
2021-06-29 13:43:28.647327 (Thread-1): On test.nyc_taxi.not_null_daily_trips_num_trips: Close
2021-06-29 13:43:28.647852 (Thread-1): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:43:28.648201 (Thread-1): Runtime Error in test not_null_daily_trips_num_trips (models/schema.yml)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:748)
    Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
    	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
    	at scala.Option.orElse(Option.scala:447)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
    	at scala.collection.immutable.List.foreach(List.scala:392)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	... 16 more
    
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 263, in exception_handler
    yield
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 80, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 208, in execute
    dbt.exceptions.raise_database_error(poll_state.errorMessage)
  File "/usr/local/lib/python3.8/site-packages/dbt/exceptions.py", line 439, in raise_database_error
    raise DatabaseException(msg, node)
dbt.exceptions.DatabaseException: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
  	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
  	at scala.Option.orElse(Option.scala:447)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  	at scala.collection.immutable.List.foldLeft(List.scala:89)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
  	at scala.collection.immutable.List.foreach(List.scala:392)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	... 16 more
  

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/task/base.py", line 344, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/base.py", line 287, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/base.py", line 389, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/test.py", line 85, in execute
    failed_rows = self.execute_schema_test(test)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/test.py", line 62, in execute_schema_test
    res, table = self.adapter.execute(
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 227, in execute
    return self.connections.execute(
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 124, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 87, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 276, in exception_handler
    raise dbt.exceptions.RuntimeException(str(exc))
dbt.exceptions.RuntimeException: Runtime Error in test not_null_daily_trips_num_trips (models/schema.yml)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:748)
    Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
    	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
    	at scala.Option.orElse(Option.scala:447)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
    	at scala.collection.immutable.List.foreach(List.scala:392)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	... 16 more
2021-06-29 13:43:28.649319 (Thread-1): 13:43:28 | 2 of 3 ERROR not_null_daily_trips_num_trips.......................... [ERROR in 0.05s]
2021-06-29 13:43:28.649467 (Thread-1): Finished running node test.nyc_taxi.not_null_daily_trips_num_trips
2021-06-29 13:43:28.649614 (Thread-1): Began running node test.nyc_taxi.unique_daily_trips_date
2021-06-29 13:43:28.649945 (Thread-1): 13:43:28 | 3 of 3 START test unique_daily_trips_date............................ [RUN]
2021-06-29 13:43:28.650233 (Thread-1): Acquiring new spark connection "test.nyc_taxi.unique_daily_trips_date".
2021-06-29 13:43:28.650367 (Thread-1): unclosed <socket.socket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.64.25', 51358), raddr=('192.168.64.6', 10000)>
2021-06-29 13:43:28.650492 (Thread-1): Compiling test.nyc_taxi.unique_daily_trips_date
2021-06-29 13:43:28.659138 (Thread-1): Writing injected SQL for node "test.nyc_taxi.unique_daily_trips_date"
2021-06-29 13:43:28.659404 (Thread-1): finished collecting timing info
2021-06-29 13:43:28.659574 (Thread-1): NotImplemented: add_begin_query
2021-06-29 13:43:28.659658 (Thread-1): Using spark connection "test.nyc_taxi.unique_daily_trips_date".
2021-06-29 13:43:28.659740 (Thread-1): On test.nyc_taxi.unique_daily_trips_date: /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "node_id": "test.nyc_taxi.unique_daily_trips_date"} */

    
    



select count(*) as validation_errors
from (

    select
        date

    from nyc_taxi_analytics.daily_trips
    where date is not null
    group by date
    having count(*) > 1

) validation_errors



2021-06-29 13:43:28.659828 (Thread-1): Opening a new connection, currently in state closed
2021-06-29 13:43:28.671460 (Thread-1): TOpenSessionResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), serverProtocolVersion=5, sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'D\xb5^1U\xbdK<\x90\r\xea1\x0e\xc9\t\xc8', secret=b'#\xbd\x91\x8a\x9d\x16A=\x8a\xbc\x9c\x9c\x86\x12*\xbc')), configuration={})
2021-06-29 13:43:28.671788 (Thread-1): USE `default`
2021-06-29 13:43:28.671970 (Thread-1): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'D\xb5^1U\xbdK<\x90\r\xea1\x0e\xc9\t\xc8', secret=b'#\xbd\x91\x8a\x9d\x16A=\x8a\xbc\x9c\x9c\x86\x12*\xbc')), statement='USE `default`', confOverlay=None, runAsync=False, queryTimeout=0)
2021-06-29 13:43:28.691751 (Thread-1): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'W\xc0\xfbT\x0e\x0fH\x84\xac\xcc66\xd0UM\r', secret=b'\xb1<\x80@\xf6\x96G\xd2\xb1\xe9\x89\x05*O\x96v'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:43:28.692885 (Thread-1): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:43:28.693128 (Thread-1): /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "node_id": "test.nyc_taxi.unique_daily_trips_date"} */

    
    



select count(*) as validation_errors
from (

    select
        date

    from nyc_taxi_analytics.daily_trips
    where date is not null
    group by date
    having count(*) > 1

) validation_errors



2021-06-29 13:43:28.693243 (Thread-1): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'D\xb5^1U\xbdK<\x90\r\xea1\x0e\xc9\t\xc8', secret=b'#\xbd\x91\x8a\x9d\x16A=\x8a\xbc\x9c\x9c\x86\x12*\xbc')), statement='/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "node_id": "test.nyc_taxi.unique_daily_trips_date"} */\n\n    \n    \n\n\n\nselect count(*) as validation_errors\nfrom (\n\n    select\n        date\n\n    from nyc_taxi_analytics.daily_trips\n    where date is not null\n    group by date\n    having count(*) > 1\n\n) validation_errors\n\n\n', confOverlay=None, runAsync=True, queryTimeout=0)
2021-06-29 13:43:28.695514 (Thread-1): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\xbe\t\xa0H\xa2SLt\x92\xbcu\x19\x180\x9fU', secret=b"\xa0\xc3\xaa\x04X\xea@\xa9\x8f\xae\xaa\xe0\x98'jP"), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:43:28.722221 (Thread-1): TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:43:28.722602 (Thread-1): Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)\n\tat org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:43:28.722707 (Thread-1): Poll status: 5
2021-06-29 13:43:28.722785 (Thread-1): Error while running:
/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "node_id": "test.nyc_taxi.unique_daily_trips_date"} */

    
    



select count(*) as validation_errors
from (

    select
        date

    from nyc_taxi_analytics.daily_trips
    where date is not null
    group by date
    having count(*) > 1

) validation_errors



2021-06-29 13:43:28.722843 (Thread-1): Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
  	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
  	at scala.Option.orElse(Option.scala:447)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  	at scala.collection.immutable.List.foldLeft(List.scala:89)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
  	at scala.collection.immutable.List.foreach(List.scala:392)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	... 16 more
  
2021-06-29 13:43:28.723083 (Thread-1): finished collecting timing info
2021-06-29 13:43:28.723219 (Thread-1): On test.nyc_taxi.unique_daily_trips_date: ROLLBACK
2021-06-29 13:43:28.723283 (Thread-1): NotImplemented: rollback
2021-06-29 13:43:28.723342 (Thread-1): On test.nyc_taxi.unique_daily_trips_date: Close
2021-06-29 13:43:28.724048 (Thread-1): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:43:28.724415 (Thread-1): Runtime Error in test unique_daily_trips_date (models/schema.yml)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:748)
    Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
    	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
    	at scala.Option.orElse(Option.scala:447)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
    	at scala.collection.immutable.List.foreach(List.scala:392)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	... 16 more
    
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 263, in exception_handler
    yield
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 80, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 208, in execute
    dbt.exceptions.raise_database_error(poll_state.errorMessage)
  File "/usr/local/lib/python3.8/site-packages/dbt/exceptions.py", line 439, in raise_database_error
    raise DatabaseException(msg, node)
dbt.exceptions.DatabaseException: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
  	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
  	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
  	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
  	at scala.Option.orElse(Option.scala:447)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  	at scala.collection.immutable.List.foldLeft(List.scala:89)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
  	at scala.collection.immutable.List.foreach(List.scala:392)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	... 16 more
  

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/task/base.py", line 344, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/base.py", line 287, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/base.py", line 389, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/test.py", line 85, in execute
    failed_rows = self.execute_schema_test(test)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/test.py", line 62, in execute_schema_test
    res, table = self.adapter.execute(
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 227, in execute
    return self.connections.execute(
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 124, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 87, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 276, in exception_handler
    raise dbt.exceptions.RuntimeException(str(exc))
dbt.exceptions.RuntimeException: Runtime Error in test unique_daily_trips_date (models/schema.yml)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:748)
    Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
    	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
    	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
    	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
    	at scala.Option.orElse(Option.scala:447)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
    	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
    	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    	at scala.collection.immutable.List.foldLeft(List.scala:89)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
    	at scala.collection.immutable.List.foreach(List.scala:392)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
    	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	... 16 more
2021-06-29 13:43:28.725062 (Thread-1): 13:43:28 | 3 of 3 ERROR unique_daily_trips_date................................. [ERROR in 0.07s]
2021-06-29 13:43:28.725186 (Thread-1): Finished running node test.nyc_taxi.unique_daily_trips_date
2021-06-29 13:43:28.726084 (MainThread): Acquiring new spark connection "master".
2021-06-29 13:43:28.726335 (MainThread): 13:43:28 | 
2021-06-29 13:43:28.726432 (MainThread): 13:43:28 | Finished running 3 tests in 0.33s.
2021-06-29 13:43:28.726513 (MainThread): Connection 'master' was properly closed.
2021-06-29 13:43:28.726574 (MainThread): Connection 'test.nyc_taxi.unique_daily_trips_date' was properly closed.
2021-06-29 13:43:28.726705 (MainThread): unclosed <socket.socket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.64.25', 51364), raddr=('192.168.64.6', 10000)>
2021-06-29 13:43:28.730911 (MainThread): 
2021-06-29 13:43:28.731063 (MainThread): Completed with 3 errors and 0 warnings:
2021-06-29 13:43:28.731154 (MainThread): 
2021-06-29 13:43:28.731260 (MainThread): Runtime Error in test not_null_daily_trips_date (models/schema.yml)
2021-06-29 13:43:28.731342 (MainThread):   Database Error
2021-06-29 13:43:28.731414 (MainThread):     org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
2021-06-29 13:43:28.731491 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
2021-06-29 13:43:28.731561 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
2021-06-29 13:43:28.731629 (MainThread):     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2021-06-29 13:43:28.731694 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
2021-06-29 13:43:28.731755 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
2021-06-29 13:43:28.731816 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
2021-06-29 13:43:28.731884 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
2021-06-29 13:43:28.731950 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
2021-06-29 13:43:28.732015 (MainThread):     	at java.security.AccessController.doPrivileged(Native Method)
2021-06-29 13:43:28.732083 (MainThread):     	at javax.security.auth.Subject.doAs(Subject.java:422)
2021-06-29 13:43:28.732149 (MainThread):     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
2021-06-29 13:43:28.732216 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
2021-06-29 13:43:28.732284 (MainThread):     	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-06-29 13:43:28.732350 (MainThread):     	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-06-29 13:43:28.732416 (MainThread):     	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-06-29 13:43:28.732482 (MainThread):     	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-06-29 13:43:28.732544 (MainThread):     	at java.lang.Thread.run(Thread.java:748)
2021-06-29 13:43:28.732607 (MainThread):     Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
2021-06-29 13:43:28.732694 (MainThread):     	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
2021-06-29 13:43:28.732763 (MainThread):     	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
2021-06-29 13:43:28.732829 (MainThread):     	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
2021-06-29 13:43:28.732897 (MainThread):     	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
2021-06-29 13:43:28.732963 (MainThread):     	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
2021-06-29 13:43:28.733027 (MainThread):     	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
2021-06-29 13:43:28.733091 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
2021-06-29 13:43:28.733156 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
2021-06-29 13:43:28.733220 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
2021-06-29 13:43:28.733287 (MainThread):     	at scala.Option.orElse(Option.scala:447)
2021-06-29 13:43:28.733350 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
2021-06-29 13:43:28.733423 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
2021-06-29 13:43:28.733489 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
2021-06-29 13:43:28.733554 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
2021-06-29 13:43:28.733618 (MainThread):     	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
2021-06-29 13:43:28.733680 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
2021-06-29 13:43:28.733743 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
2021-06-29 13:43:28.733805 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
2021-06-29 13:43:28.733868 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
2021-06-29 13:43:28.733930 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
2021-06-29 13:43:28.733992 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
2021-06-29 13:43:28.734053 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
2021-06-29 13:43:28.734121 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
2021-06-29 13:43:28.734184 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
2021-06-29 13:43:28.734246 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
2021-06-29 13:43:28.734308 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
2021-06-29 13:43:28.734371 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
2021-06-29 13:43:28.734432 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
2021-06-29 13:43:28.734495 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
2021-06-29 13:43:28.734559 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
2021-06-29 13:43:28.734621 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
2021-06-29 13:43:28.734683 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
2021-06-29 13:43:28.734745 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
2021-06-29 13:43:28.734805 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
2021-06-29 13:43:28.734866 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
2021-06-29 13:43:28.734930 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
2021-06-29 13:43:28.734992 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
2021-06-29 13:43:28.735054 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
2021-06-29 13:43:28.735123 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
2021-06-29 13:43:28.735187 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
2021-06-29 13:43:28.735247 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
2021-06-29 13:43:28.735308 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
2021-06-29 13:43:28.735371 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
2021-06-29 13:43:28.735433 (MainThread):     	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
2021-06-29 13:43:28.735495 (MainThread):     	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
2021-06-29 13:43:28.735556 (MainThread):     	at scala.collection.immutable.List.foldLeft(List.scala:89)
2021-06-29 13:43:28.735618 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
2021-06-29 13:43:28.735682 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
2021-06-29 13:43:28.735746 (MainThread):     	at scala.collection.immutable.List.foreach(List.scala:392)
2021-06-29 13:43:28.735810 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
2021-06-29 13:43:28.735873 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
2021-06-29 13:43:28.735937 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
2021-06-29 13:43:28.736000 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
2021-06-29 13:43:28.736062 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
2021-06-29 13:43:28.736123 (MainThread):     	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
2021-06-29 13:43:28.736184 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
2021-06-29 13:43:28.736246 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
2021-06-29 13:43:28.736307 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
2021-06-29 13:43:28.736371 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
2021-06-29 13:43:28.736433 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
2021-06-29 13:43:28.736496 (MainThread):     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
2021-06-29 13:43:28.736557 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
2021-06-29 13:43:28.736621 (MainThread):     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
2021-06-29 13:43:28.736692 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
2021-06-29 13:43:28.736759 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
2021-06-29 13:43:28.736823 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
2021-06-29 13:43:28.736888 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
2021-06-29 13:43:28.736958 (MainThread):     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
2021-06-29 13:43:28.737024 (MainThread):     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
2021-06-29 13:43:28.737089 (MainThread):     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
2021-06-29 13:43:28.737155 (MainThread):     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
2021-06-29 13:43:28.737220 (MainThread):     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
2021-06-29 13:43:28.737285 (MainThread):     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
2021-06-29 13:43:28.737350 (MainThread):     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
2021-06-29 13:43:28.737412 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
2021-06-29 13:43:28.737478 (MainThread):     	... 16 more
2021-06-29 13:43:28.737543 (MainThread):     Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.sources.DeltaSourceUtils$
2021-06-29 13:43:28.737607 (MainThread):     	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
2021-06-29 13:43:28.737672 (MainThread):     	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
2021-06-29 13:43:28.737739 (MainThread):     	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
2021-06-29 13:43:28.737804 (MainThread):     	... 91 more
2021-06-29 13:43:28.737867 (MainThread):     
2021-06-29 13:43:28.737943 (MainThread): 
2021-06-29 13:43:28.738045 (MainThread): Runtime Error in test not_null_daily_trips_num_trips (models/schema.yml)
2021-06-29 13:43:28.738116 (MainThread):   Database Error
2021-06-29 13:43:28.738181 (MainThread):     org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
2021-06-29 13:43:28.738245 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
2021-06-29 13:43:28.738310 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
2021-06-29 13:43:28.738375 (MainThread):     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2021-06-29 13:43:28.738439 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
2021-06-29 13:43:28.738504 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
2021-06-29 13:43:28.738569 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
2021-06-29 13:43:28.738631 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
2021-06-29 13:43:28.738694 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
2021-06-29 13:43:28.738756 (MainThread):     	at java.security.AccessController.doPrivileged(Native Method)
2021-06-29 13:43:28.738819 (MainThread):     	at javax.security.auth.Subject.doAs(Subject.java:422)
2021-06-29 13:43:28.738881 (MainThread):     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
2021-06-29 13:43:28.738945 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
2021-06-29 13:43:28.739008 (MainThread):     	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-06-29 13:43:28.739079 (MainThread):     	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-06-29 13:43:28.739143 (MainThread):     	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-06-29 13:43:28.739209 (MainThread):     	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-06-29 13:43:28.739272 (MainThread):     	at java.lang.Thread.run(Thread.java:748)
2021-06-29 13:43:28.739337 (MainThread):     Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
2021-06-29 13:43:28.739399 (MainThread):     	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
2021-06-29 13:43:28.739465 (MainThread):     	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
2021-06-29 13:43:28.739529 (MainThread):     	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
2021-06-29 13:43:28.739594 (MainThread):     	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
2021-06-29 13:43:28.739658 (MainThread):     	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
2021-06-29 13:43:28.739721 (MainThread):     	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
2021-06-29 13:43:28.739784 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
2021-06-29 13:43:28.739847 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
2021-06-29 13:43:28.739909 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
2021-06-29 13:43:28.739972 (MainThread):     	at scala.Option.orElse(Option.scala:447)
2021-06-29 13:43:28.740035 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
2021-06-29 13:43:28.740098 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
2021-06-29 13:43:28.740161 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
2021-06-29 13:43:28.740222 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
2021-06-29 13:43:28.740284 (MainThread):     	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
2021-06-29 13:43:28.740346 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
2021-06-29 13:43:28.740407 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
2021-06-29 13:43:28.740469 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
2021-06-29 13:43:28.740531 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
2021-06-29 13:43:28.740593 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
2021-06-29 13:43:28.740674 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
2021-06-29 13:43:28.740743 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
2021-06-29 13:43:28.740807 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
2021-06-29 13:43:28.740870 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
2021-06-29 13:43:28.740941 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
2021-06-29 13:43:28.741005 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
2021-06-29 13:43:28.741067 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
2021-06-29 13:43:28.741128 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
2021-06-29 13:43:28.741190 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
2021-06-29 13:43:28.741251 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
2021-06-29 13:43:28.741313 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
2021-06-29 13:43:28.741376 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
2021-06-29 13:43:28.741439 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
2021-06-29 13:43:28.741504 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
2021-06-29 13:43:28.741566 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
2021-06-29 13:43:28.741629 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
2021-06-29 13:43:28.741691 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
2021-06-29 13:43:28.741753 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
2021-06-29 13:43:28.741817 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
2021-06-29 13:43:28.741881 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
2021-06-29 13:43:28.741944 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
2021-06-29 13:43:28.742006 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
2021-06-29 13:43:28.742071 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
2021-06-29 13:43:28.742136 (MainThread):     	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
2021-06-29 13:43:28.742198 (MainThread):     	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
2021-06-29 13:43:28.742262 (MainThread):     	at scala.collection.immutable.List.foldLeft(List.scala:89)
2021-06-29 13:43:28.742325 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
2021-06-29 13:43:28.742387 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
2021-06-29 13:43:28.742449 (MainThread):     	at scala.collection.immutable.List.foreach(List.scala:392)
2021-06-29 13:43:28.742512 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
2021-06-29 13:43:28.742575 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
2021-06-29 13:43:28.742640 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
2021-06-29 13:43:28.742703 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
2021-06-29 13:43:28.742773 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
2021-06-29 13:43:28.742839 (MainThread):     	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
2021-06-29 13:43:28.742906 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
2021-06-29 13:43:28.742971 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
2021-06-29 13:43:28.743037 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
2021-06-29 13:43:28.743101 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
2021-06-29 13:43:28.743166 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
2021-06-29 13:43:28.743231 (MainThread):     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
2021-06-29 13:43:28.743296 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
2021-06-29 13:43:28.743360 (MainThread):     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
2021-06-29 13:43:28.743424 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
2021-06-29 13:43:28.743486 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
2021-06-29 13:43:28.743549 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
2021-06-29 13:43:28.743611 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
2021-06-29 13:43:28.743674 (MainThread):     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
2021-06-29 13:43:28.743737 (MainThread):     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
2021-06-29 13:43:28.743800 (MainThread):     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
2021-06-29 13:43:28.743863 (MainThread):     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
2021-06-29 13:43:28.743926 (MainThread):     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
2021-06-29 13:43:28.743991 (MainThread):     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
2021-06-29 13:43:28.744056 (MainThread):     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
2021-06-29 13:43:28.744120 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
2021-06-29 13:43:28.744185 (MainThread):     	... 16 more
2021-06-29 13:43:28.744253 (MainThread):     
2021-06-29 13:43:28.744333 (MainThread): 
2021-06-29 13:43:28.744447 (MainThread): Runtime Error in test unique_daily_trips_date (models/schema.yml)
2021-06-29 13:43:28.744522 (MainThread):   Database Error
2021-06-29 13:43:28.744589 (MainThread):     org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
2021-06-29 13:43:28.744653 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
2021-06-29 13:43:28.744730 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
2021-06-29 13:43:28.744793 (MainThread):     	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2021-06-29 13:43:28.744864 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
2021-06-29 13:43:28.744929 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
2021-06-29 13:43:28.744995 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
2021-06-29 13:43:28.745062 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
2021-06-29 13:43:28.745127 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
2021-06-29 13:43:28.745194 (MainThread):     	at java.security.AccessController.doPrivileged(Native Method)
2021-06-29 13:43:28.745257 (MainThread):     	at javax.security.auth.Subject.doAs(Subject.java:422)
2021-06-29 13:43:28.745321 (MainThread):     	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
2021-06-29 13:43:28.745386 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
2021-06-29 13:43:28.745450 (MainThread):     	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-06-29 13:43:28.745513 (MainThread):     	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-06-29 13:43:28.745578 (MainThread):     	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-06-29 13:43:28.745642 (MainThread):     	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-06-29 13:43:28.745708 (MainThread):     	at java.lang.Thread.run(Thread.java:748)
2021-06-29 13:43:28.745773 (MainThread):     Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/sources/DeltaSourceUtils$
2021-06-29 13:43:28.745840 (MainThread):     	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.hasDeltaNamespace(DeltaCatalog.scala:570)
2021-06-29 13:43:28.745906 (MainThread):     	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier(DeltaCatalog.scala:576)
2021-06-29 13:43:28.745972 (MainThread):     	at org.apache.spark.sql.delta.catalog.SupportsPathIdentifier.isPathIdentifier$(DeltaCatalog.scala:573)
2021-06-29 13:43:28.746036 (MainThread):     	at org.apache.spark.sql.delta.catalog.DeltaCatalog.isPathIdentifier(DeltaCatalog.scala:57)
2021-06-29 13:43:28.746101 (MainThread):     	at org.apache.spark.sql.delta.catalog.DeltaCatalog.loadTable(DeltaCatalog.scala:165)
2021-06-29 13:43:28.746165 (MainThread):     	at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:282)
2021-06-29 13:43:28.746231 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1173)
2021-06-29 13:43:28.746298 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1173)
2021-06-29 13:43:28.746362 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1211)
2021-06-29 13:43:28.746428 (MainThread):     	at scala.Option.orElse(Option.scala:447)
2021-06-29 13:43:28.746493 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1210)
2021-06-29 13:43:28.746560 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1135)
2021-06-29 13:43:28.746625 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1102)
2021-06-29 13:43:28.746691 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
2021-06-29 13:43:28.746768 (MainThread):     	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
2021-06-29 13:43:28.746837 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
2021-06-29 13:43:28.746907 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
2021-06-29 13:43:28.746976 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
2021-06-29 13:43:28.747045 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
2021-06-29 13:43:28.747115 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
2021-06-29 13:43:28.747185 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
2021-06-29 13:43:28.747253 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
2021-06-29 13:43:28.747320 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
2021-06-29 13:43:28.747388 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
2021-06-29 13:43:28.747455 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
2021-06-29 13:43:28.747521 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
2021-06-29 13:43:28.747592 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
2021-06-29 13:43:28.747665 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
2021-06-29 13:43:28.747737 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
2021-06-29 13:43:28.747808 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
2021-06-29 13:43:28.747876 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
2021-06-29 13:43:28.747942 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
2021-06-29 13:43:28.748009 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
2021-06-29 13:43:28.748074 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
2021-06-29 13:43:28.748140 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
2021-06-29 13:43:28.748204 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
2021-06-29 13:43:28.748272 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
2021-06-29 13:43:28.748338 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
2021-06-29 13:43:28.748403 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
2021-06-29 13:43:28.748468 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
2021-06-29 13:43:28.748534 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
2021-06-29 13:43:28.748605 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
2021-06-29 13:43:28.748690 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
2021-06-29 13:43:28.748760 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
2021-06-29 13:43:28.748825 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
2021-06-29 13:43:28.748892 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
2021-06-29 13:43:28.748960 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
2021-06-29 13:43:28.749027 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
2021-06-29 13:43:28.749094 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
2021-06-29 13:43:28.749160 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
2021-06-29 13:43:28.749224 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
2021-06-29 13:43:28.749290 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
2021-06-29 13:43:28.749355 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
2021-06-29 13:43:28.749420 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
2021-06-29 13:43:28.749485 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
2021-06-29 13:43:28.749552 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
2021-06-29 13:43:28.749618 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
2021-06-29 13:43:28.749683 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
2021-06-29 13:43:28.749748 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
2021-06-29 13:43:28.749812 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
2021-06-29 13:43:28.749876 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
2021-06-29 13:43:28.749942 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
2021-06-29 13:43:28.750007 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
2021-06-29 13:43:28.750074 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
2021-06-29 13:43:28.750139 (MainThread):     	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
2021-06-29 13:43:28.750206 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
2021-06-29 13:43:28.750271 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
2021-06-29 13:43:28.750336 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
2021-06-29 13:43:28.750403 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
2021-06-29 13:43:28.750473 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
2021-06-29 13:43:28.750540 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1102)
2021-06-29 13:43:28.750607 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1070)
2021-06-29 13:43:28.750673 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
2021-06-29 13:43:28.750737 (MainThread):     	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
2021-06-29 13:43:28.750801 (MainThread):     	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
2021-06-29 13:43:28.750865 (MainThread):     	at scala.collection.immutable.List.foldLeft(List.scala:89)
2021-06-29 13:43:28.750930 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
2021-06-29 13:43:28.750994 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
2021-06-29 13:43:28.751058 (MainThread):     	at scala.collection.immutable.List.foreach(List.scala:392)
2021-06-29 13:43:28.751122 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
2021-06-29 13:43:28.751186 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
2021-06-29 13:43:28.751250 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
2021-06-29 13:43:28.751315 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
2021-06-29 13:43:28.751377 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
2021-06-29 13:43:28.751442 (MainThread):     	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
2021-06-29 13:43:28.751506 (MainThread):     	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
2021-06-29 13:43:28.751568 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
2021-06-29 13:43:28.751626 (MainThread):     	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
2021-06-29 13:43:28.751687 (MainThread):     	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
2021-06-29 13:43:28.751743 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
2021-06-29 13:43:28.751803 (MainThread):     	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
2021-06-29 13:43:28.751862 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
2021-06-29 13:43:28.751920 (MainThread):     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
2021-06-29 13:43:28.751979 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
2021-06-29 13:43:28.752041 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
2021-06-29 13:43:28.752102 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
2021-06-29 13:43:28.752163 (MainThread):     	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
2021-06-29 13:43:28.752223 (MainThread):     	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
2021-06-29 13:43:28.752282 (MainThread):     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
2021-06-29 13:43:28.752350 (MainThread):     	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
2021-06-29 13:43:28.752415 (MainThread):     	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
2021-06-29 13:43:28.752477 (MainThread):     	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
2021-06-29 13:43:28.752538 (MainThread):     	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
2021-06-29 13:43:28.752599 (MainThread):     	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
2021-06-29 13:43:28.752670 (MainThread):     	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
2021-06-29 13:43:28.752739 (MainThread):     	... 16 more
2021-06-29 13:43:28.752798 (MainThread):     
2021-06-29 13:43:28.752898 (MainThread): 
Done. PASS=0 WARN=0 ERROR=3 SKIP=0 TOTAL=3
2021-06-29 13:43:28.753157 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9512883d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9512ad5e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9512870400>]}
2021-06-29 13:43:28.753452 (MainThread): Flushing usage events
2021-06-29 13:43:33.669406 (MainThread): Running with dbt=0.19.1
2021-06-29 13:43:33.680976 (MainThread): the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
2021-06-29 13:43:33.734602 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/root/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-06-29 13:43:33.735207 (MainThread): Tracking: tracking
2021-06-29 13:43:33.737042 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3baef77e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3baedad580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3baedad6a0>]}
2021-06-29 13:43:33.744416 (MainThread): Partial parsing not enabled
2021-06-29 13:43:33.745152 (MainThread): Parsing macros/adapters.sql
2021-06-29 13:43:33.764242 (MainThread): Parsing macros/materializations/table.sql
2021-06-29 13:43:33.766739 (MainThread): Parsing macros/materializations/snapshot.sql
2021-06-29 13:43:33.786712 (MainThread): Parsing macros/materializations/view.sql
2021-06-29 13:43:33.787204 (MainThread): Parsing macros/materializations/seed.sql
2021-06-29 13:43:33.798818 (MainThread): Parsing macros/materializations/incremental/strategies.sql
2021-06-29 13:43:33.803620 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-06-29 13:43:33.808164 (MainThread): Parsing macros/materializations/incremental/validate.sql
2021-06-29 13:43:33.813385 (MainThread): Parsing macros/core.sql
2021-06-29 13:43:33.816649 (MainThread): Parsing macros/adapters/common.sql
2021-06-29 13:43:33.851783 (MainThread): Parsing macros/schema_tests/unique.sql
2021-06-29 13:43:33.853203 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-06-29 13:43:33.855446 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-06-29 13:43:33.856690 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-06-29 13:43:33.858231 (MainThread): Parsing macros/etc/datetime.sql
2021-06-29 13:43:33.865637 (MainThread): Parsing macros/etc/query.sql
2021-06-29 13:43:33.866507 (MainThread): Parsing macros/etc/is_incremental.sql
2021-06-29 13:43:33.867823 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-06-29 13:43:33.869166 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-06-29 13:43:33.869903 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-06-29 13:43:33.871484 (MainThread): Parsing macros/materializations/helpers.sql
2021-06-29 13:43:33.878737 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-06-29 13:43:33.893669 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-06-29 13:43:33.918443 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-06-29 13:43:33.919888 (MainThread): Parsing macros/materializations/table/table.sql
2021-06-29 13:43:33.925423 (MainThread): Parsing macros/materializations/common/merge.sql
2021-06-29 13:43:33.936892 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-06-29 13:43:33.938412 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-06-29 13:43:33.971017 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-06-29 13:43:33.988326 (MainThread): Parsing macros/materializations/view/view.sql
2021-06-29 13:43:33.993667 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-06-29 13:43:34.001840 (MainThread): Partial parsing not enabled
2021-06-29 13:43:34.021804 (MainThread): Acquiring new spark connection "model.nyc_taxi.daily_trips".
2021-06-29 13:43:34.077882 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5d0b5f9c-bbf5-4d3d-8d21-f1634f2bbb47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3baeb17910>]}
2021-06-29 13:43:34.081391 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5d0b5f9c-bbf5-4d3d-8d21-f1634f2bbb47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3baeb7ea90>]}
2021-06-29 13:43:34.081560 (MainThread): Found 1 model, 3 tests, 0 snapshots, 0 analyses, 158 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-06-29 13:43:34.082042 (MainThread): 
2021-06-29 13:43:34.082244 (MainThread): Acquiring new spark connection "master".
2021-06-29 13:43:34.083158 (ThreadPoolExecutor-0_0): Acquiring new spark connection "list_schemas".
2021-06-29 13:43:34.095665 (ThreadPoolExecutor-0_0): Using spark connection "list_schemas".
2021-06-29 13:43:34.095781 (ThreadPoolExecutor-0_0): On list_schemas: /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
2021-06-29 13:43:34.095850 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-06-29 13:43:34.110718 (ThreadPoolExecutor-0_0): TOpenSessionResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), serverProtocolVersion=5, sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'y\xc5\xd8\xc4l\xccG\xa0\xbd\xe9\xc7\xf3\x91\xdel:', secret=b'\xd4\x87GpJ\xeeJ\xde\xbe\xac\xea\x19\x86}\xdd\xe2')), configuration={})
2021-06-29 13:43:34.111168 (ThreadPoolExecutor-0_0): USE `default`
2021-06-29 13:43:34.111292 (ThreadPoolExecutor-0_0): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'y\xc5\xd8\xc4l\xccG\xa0\xbd\xe9\xc7\xf3\x91\xdel:', secret=b'\xd4\x87GpJ\xeeJ\xde\xbe\xac\xea\x19\x86}\xdd\xe2')), statement='USE `default`', confOverlay=None, runAsync=False, queryTimeout=0)
2021-06-29 13:43:34.128096 (ThreadPoolExecutor-0_0): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\xbe\xf2\x8b\xa7JYM+\xac@\x9e}\xe1|3O', secret=b'9(\x8c\x82-#F<\xa4a\x05\x08\xb3\xb4\xad/'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:43:34.129429 (ThreadPoolExecutor-0_0): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:43:34.129658 (ThreadPoolExecutor-0_0): /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
2021-06-29 13:43:34.129798 (ThreadPoolExecutor-0_0): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'y\xc5\xd8\xc4l\xccG\xa0\xbd\xe9\xc7\xf3\x91\xdel:', secret=b'\xd4\x87GpJ\xeeJ\xde\xbe\xac\xea\x19\x86}\xdd\xe2')), statement='/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "list_schemas"} */\n\n    show databases\n  ', confOverlay=None, runAsync=True, queryTimeout=0)
2021-06-29 13:43:34.131336 (ThreadPoolExecutor-0_0): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'w{%\x95<hF\xd0\x85\xc3R\x84f\xca\x1b\xb4', secret=b'\xc6\xb1rQ\x90TL\x9c\x8a\x8f\xb1=\xae\x1c\xfd\xde'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:43:34.204478 (ThreadPoolExecutor-0_0): TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=2, sqlState=None, errorCode=None, errorMessage=None, taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:43:34.204635 (ThreadPoolExecutor-0_0): Poll status: 2, query complete
2021-06-29 13:43:34.204702 (ThreadPoolExecutor-0_0): SQL status: OK in 0.11 seconds
2021-06-29 13:43:34.206609 (ThreadPoolExecutor-0_0): TGetResultSetMetadataResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), schema=TTableSchema(columns=[TColumnDesc(columnName='namespace', typeDesc=TTypeDesc(types=[TTypeEntry(primitiveEntry=TPrimitiveTypeEntry(type=7, typeQualifiers=None), arrayEntry=None, mapEntry=None, structEntry=None, unionEntry=None, userDefinedTypeEntry=None)]), position=1, comment='')]))
2021-06-29 13:43:34.207385 (ThreadPoolExecutor-0_0): TFetchResultsResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), hasMoreRows=False, results=TRowSet(startRowOffset=0, rows=[], columns=[TColumn(boolVal=None, byteVal=None, i16Val=None, i32Val=None, i64Val=None, doubleVal=None, stringVal=TStringColumn(values=['clean', 'default', 'processed', 'raw'], nulls=b'\x00'), binaryVal=None)], binaryColumns=None, columnCount=None))
2021-06-29 13:43:34.207797 (ThreadPoolExecutor-0_0): TFetchResultsResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), hasMoreRows=False, results=TRowSet(startRowOffset=4, rows=[], columns=[TColumn(boolVal=None, byteVal=None, i16Val=None, i32Val=None, i64Val=None, doubleVal=None, stringVal=TStringColumn(values=[], nulls=b'\x00'), binaryVal=None)], binaryColumns=None, columnCount=None))
2021-06-29 13:43:34.209766 (ThreadPoolExecutor-0_0): On list_schemas: Close
2021-06-29 13:43:34.210249 (ThreadPoolExecutor-0_0): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:43:34.210796 (ThreadPoolExecutor-0_0): Acquiring new spark connection "create__nyc_taxi_analytics".
2021-06-29 13:43:34.210931 (ThreadPoolExecutor-0_0): unclosed <socket.socket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.64.25', 51376), raddr=('192.168.64.6', 10000)>
2021-06-29 13:43:34.211144 (ThreadPoolExecutor-0_0): Acquiring new spark connection "create__nyc_taxi_analytics".
2021-06-29 13:43:34.211284 (ThreadPoolExecutor-0_0): Creating schema "nyc_taxi_analytics"
2021-06-29 13:43:34.216507 (ThreadPoolExecutor-0_0): NotImplemented: add_begin_query
2021-06-29 13:43:34.216603 (ThreadPoolExecutor-0_0): Using spark connection "create__nyc_taxi_analytics".
2021-06-29 13:43:34.216690 (ThreadPoolExecutor-0_0): On create__nyc_taxi_analytics: /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "create__nyc_taxi_analytics"} */
create schema if not exists nyc_taxi_analytics
  
2021-06-29 13:43:34.216788 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state closed
2021-06-29 13:43:34.228899 (ThreadPoolExecutor-0_0): TOpenSessionResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), serverProtocolVersion=5, sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\xe0U\xd0\x9f\xdbmFw\x8f\xbf\x19\xfe\xd1\xb9\xa6)', secret=b'\xc5B\xcd\xf5E\xb7G\xe8\x9dQ\xabF?\xdf\\\xf7')), configuration={})
2021-06-29 13:43:34.229160 (ThreadPoolExecutor-0_0): USE `default`
2021-06-29 13:43:34.229276 (ThreadPoolExecutor-0_0): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\xe0U\xd0\x9f\xdbmFw\x8f\xbf\x19\xfe\xd1\xb9\xa6)', secret=b'\xc5B\xcd\xf5E\xb7G\xe8\x9dQ\xabF?\xdf\\\xf7')), statement='USE `default`', confOverlay=None, runAsync=False, queryTimeout=0)
2021-06-29 13:43:34.246068 (ThreadPoolExecutor-0_0): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\x0e\x8b\xfc\xbcU\xe3J\xe4\x8d\xe6\xe9~>p\x04\x93', secret=b"\x82\xa2\xa3\xdd\xaa'H\xd5\xbb\xee\xda\xc0J\xe2\xe33"), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:43:34.246981 (ThreadPoolExecutor-0_0): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:43:34.247208 (ThreadPoolExecutor-0_0): /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "create__nyc_taxi_analytics"} */
create schema if not exists nyc_taxi_analytics
  
2021-06-29 13:43:34.247350 (ThreadPoolExecutor-0_0): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\xe0U\xd0\x9f\xdbmFw\x8f\xbf\x19\xfe\xd1\xb9\xa6)', secret=b'\xc5B\xcd\xf5E\xb7G\xe8\x9dQ\xabF?\xdf\\\xf7')), statement='/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "create__nyc_taxi_analytics"} */\ncreate schema if not exists nyc_taxi_analytics\n  ', confOverlay=None, runAsync=True, queryTimeout=0)
2021-06-29 13:43:34.249120 (ThreadPoolExecutor-0_0): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'{z\x043XHD\x7f\x87\xf3Z\xa4\xb4\xd2o\xc1', secret=b'9\xcc\x06*\xd3,K_\xa5>cb\xa1\x95\x9d\x10'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:43:34.264401 (ThreadPoolExecutor-0_0): TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)\n\tat org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\t... 16 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 47 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:43:34.264584 (ThreadPoolExecutor-0_0): Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)\n\tat org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\t... 16 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 47 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:43:34.264672 (ThreadPoolExecutor-0_0): Poll status: 5
2021-06-29 13:43:34.264752 (ThreadPoolExecutor-0_0): Error while running:
/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "create__nyc_taxi_analytics"} */
create schema if not exists nyc_taxi_analytics
  
2021-06-29 13:43:34.264810 (ThreadPoolExecutor-0_0): Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
  	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
  	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
  	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
  	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
  	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
  	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
  	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
  	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	... 16 more
  Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
  	... 47 more
  
2021-06-29 13:43:34.264999 (ThreadPoolExecutor-0_0): Error while running:
macro create_schema
2021-06-29 13:43:34.265058 (ThreadPoolExecutor-0_0): Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:748)
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
    	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
    	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
    	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
    	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
    	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	... 16 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
    	... 47 more
    
2021-06-29 13:43:34.265219 (ThreadPoolExecutor-0_0): On create__nyc_taxi_analytics: ROLLBACK
2021-06-29 13:43:34.265279 (ThreadPoolExecutor-0_0): NotImplemented: rollback
2021-06-29 13:43:34.265333 (ThreadPoolExecutor-0_0): On create__nyc_taxi_analytics: Close
2021-06-29 13:43:34.265863 (ThreadPoolExecutor-0_0): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:43:34.266643 (MainThread): Connection 'master' was properly closed.
2021-06-29 13:43:34.266752 (MainThread): Connection 'create__nyc_taxi_analytics' was properly closed.
2021-06-29 13:43:34.266935 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3baec03d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3baec701c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3baeb145e0>]}
2021-06-29 13:43:34.267121 (MainThread): Flushing usage events
2021-06-29 13:43:35.208181 (MainThread): Encountered an error:
2021-06-29 13:43:35.208825 (MainThread): Runtime Error
  Runtime Error
    Database Error
      org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
      	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
      	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
      	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
      	at java.security.AccessController.doPrivileged(Native Method)
      	at javax.security.auth.Subject.doAs(Subject.java:422)
      	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
      	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
      	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
      	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
      	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
      	at java.lang.Thread.run(Thread.java:748)
      Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
      	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
      	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
      	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
      	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
      	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
      	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
      	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
      	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
      	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
      	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
      	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
      	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
      	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
      	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
      	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
      	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
      	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
      	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
      	... 16 more
      Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
      	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
      	... 47 more
      
2021-06-29 13:43:35.212306 (MainThread): Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 263, in exception_handler
    yield
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 80, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 208, in execute
    dbt.exceptions.raise_database_error(poll_state.errorMessage)
  File "/usr/local/lib/python3.8/site-packages/dbt/exceptions.py", line 439, in raise_database_error
    raise DatabaseException(msg, node)
dbt.exceptions.DatabaseException: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
  	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
  	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
  	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
  	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
  	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
  	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
  	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
  	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	... 16 more
  Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
  	... 47 more
  

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 263, in exception_handler
    yield
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 1002, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 332, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 259, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 19, in macro
  File "/usr/local/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 332, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 259, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 28, in macro
  File "/usr/local/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 332, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 259, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 227, in execute
    return self.connections.execute(
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 124, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 87, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 276, in exception_handler
    raise dbt.exceptions.RuntimeException(str(exc))
dbt.exceptions.RuntimeException: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:748)
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
    	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
    	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
    	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
    	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
    	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	... 16 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
    	... 47 more
    

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/main.py", line 125, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/lib/python3.8/site-packages/dbt/main.py", line 203, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/lib/python3.8/site-packages/dbt/main.py", line 256, in run_from_args
    results = task.run()
  File "/usr/local/lib/python3.8/site-packages/dbt/task/runnable.py", line 426, in run
    result = self.execute_with_hooks(selected_uids)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/runnable.py", line 383, in execute_with_hooks
    self.before_run(adapter, selected_uids)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/run.py", line 409, in before_run
    self.create_schemas(adapter, selected_uids)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/runnable.py", line 538, in create_schemas
    create_future.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.8/concurrent/futures/thread.py", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/utils.py", line 469, in connected
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/runnable.py", line 500, in create_schema
    adapter.create_schema(relation)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/impl.py", line 182, in create_schema
    self.execute_macro(CREATE_SCHEMA_MACRO_NAME, kwargs=kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 1002, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 276, in exception_handler
    raise dbt.exceptions.RuntimeException(str(exc))
dbt.exceptions.RuntimeException: Runtime Error
  Runtime Error
    Database Error
      org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
      	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
      	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
      	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
      	at java.security.AccessController.doPrivileged(Native Method)
      	at javax.security.auth.Subject.doAs(Subject.java:422)
      	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
      	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
      	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
      	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
      	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
      	at java.lang.Thread.run(Thread.java:748)
      Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
      	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
      	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
      	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
      	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
      	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
      	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
      	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
      	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
      	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
      	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
      	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
      	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
      	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
      	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
      	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
      	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
      	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
      	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
      	... 16 more
      Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
      	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
      	... 47 more
      

2021-06-29 13:44:28.275050 (MainThread): Running with dbt=0.19.1
2021-06-29 13:44:28.286430 (MainThread): the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
2021-06-29 13:44:28.341717 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/root/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-06-29 13:44:28.342218 (MainThread): Tracking: tracking
2021-06-29 13:44:28.343923 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4897da6eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4897bdb5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4897bdb700>]}
2021-06-29 13:44:28.351141 (MainThread): Partial parsing not enabled
2021-06-29 13:44:28.351907 (MainThread): Parsing macros/adapters.sql
2021-06-29 13:44:28.371500 (MainThread): Parsing macros/materializations/table.sql
2021-06-29 13:44:28.374061 (MainThread): Parsing macros/materializations/snapshot.sql
2021-06-29 13:44:28.393862 (MainThread): Parsing macros/materializations/view.sql
2021-06-29 13:44:28.394351 (MainThread): Parsing macros/materializations/seed.sql
2021-06-29 13:44:28.405963 (MainThread): Parsing macros/materializations/incremental/strategies.sql
2021-06-29 13:44:28.410649 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-06-29 13:44:28.415136 (MainThread): Parsing macros/materializations/incremental/validate.sql
2021-06-29 13:44:28.420385 (MainThread): Parsing macros/core.sql
2021-06-29 13:44:28.423648 (MainThread): Parsing macros/adapters/common.sql
2021-06-29 13:44:28.458840 (MainThread): Parsing macros/schema_tests/unique.sql
2021-06-29 13:44:28.460292 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-06-29 13:44:28.462563 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-06-29 13:44:28.463790 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-06-29 13:44:28.465335 (MainThread): Parsing macros/etc/datetime.sql
2021-06-29 13:44:28.472842 (MainThread): Parsing macros/etc/query.sql
2021-06-29 13:44:28.473695 (MainThread): Parsing macros/etc/is_incremental.sql
2021-06-29 13:44:28.475010 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-06-29 13:44:28.476337 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-06-29 13:44:28.477100 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-06-29 13:44:28.478679 (MainThread): Parsing macros/materializations/helpers.sql
2021-06-29 13:44:28.485948 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-06-29 13:44:28.500813 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-06-29 13:44:28.526277 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-06-29 13:44:28.527704 (MainThread): Parsing macros/materializations/table/table.sql
2021-06-29 13:44:28.533222 (MainThread): Parsing macros/materializations/common/merge.sql
2021-06-29 13:44:28.544644 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-06-29 13:44:28.546432 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-06-29 13:44:28.575015 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-06-29 13:44:28.591906 (MainThread): Parsing macros/materializations/view/view.sql
2021-06-29 13:44:28.597109 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-06-29 13:44:28.607150 (MainThread): Partial parsing not enabled
2021-06-29 13:44:28.628594 (MainThread): Acquiring new spark connection "model.nyc_taxi.daily_trips".
2021-06-29 13:44:28.682818 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1751e130-e786-48a8-84ef-63fcd94d466e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4897946910>]}
2021-06-29 13:44:28.686123 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1751e130-e786-48a8-84ef-63fcd94d466e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48979adaf0>]}
2021-06-29 13:44:28.686260 (MainThread): Found 1 model, 3 tests, 0 snapshots, 0 analyses, 158 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-06-29 13:44:28.686715 (MainThread): 
2021-06-29 13:44:28.686895 (MainThread): Acquiring new spark connection "master".
2021-06-29 13:44:28.687818 (ThreadPoolExecutor-0_0): Acquiring new spark connection "list_schemas".
2021-06-29 13:44:28.699598 (ThreadPoolExecutor-0_0): Using spark connection "list_schemas".
2021-06-29 13:44:28.699706 (ThreadPoolExecutor-0_0): On list_schemas: /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
2021-06-29 13:44:28.699769 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-06-29 13:44:29.016808 (ThreadPoolExecutor-0_0): TOpenSessionResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), serverProtocolVersion=5, sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\x02-\\$YfLp\xadBu\n\xbf\x94\xb5Y', secret=b'A\x97\x08W\xaa=E\x01\x8a\x9a\xdf\xf9M\x04\xcc\xf3')), configuration={})
2021-06-29 13:44:29.017216 (ThreadPoolExecutor-0_0): USE `default`
2021-06-29 13:44:29.017309 (ThreadPoolExecutor-0_0): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\x02-\\$YfLp\xadBu\n\xbf\x94\xb5Y', secret=b'A\x97\x08W\xaa=E\x01\x8a\x9a\xdf\xf9M\x04\xcc\xf3')), statement='USE `default`', confOverlay=None, runAsync=False, queryTimeout=0)
2021-06-29 13:44:30.291388 (ThreadPoolExecutor-0_0): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\xa7{]\x8b\xaf\xa6E_\x80\x80{,\xd3\x07\x10T', secret=b'\x83d\x9f\x9e\xdajO\x9f\xa6\xbfj\xf5\xc7\x97\xee\xa9'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:44:30.300015 (ThreadPoolExecutor-0_0): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:44:30.300264 (ThreadPoolExecutor-0_0): /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
2021-06-29 13:44:30.300358 (ThreadPoolExecutor-0_0): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\x02-\\$YfLp\xadBu\n\xbf\x94\xb5Y', secret=b'A\x97\x08W\xaa=E\x01\x8a\x9a\xdf\xf9M\x04\xcc\xf3')), statement='/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "list_schemas"} */\n\n    show databases\n  ', confOverlay=None, runAsync=True, queryTimeout=0)
2021-06-29 13:44:30.303204 (ThreadPoolExecutor-0_0): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\x86\xc0\x1b\xe0\x15\x8c@4\x97\x17R\xd5\xf2\xbbH\xb1', secret=b'\xea\xf7\xa0\xfbF\xddN\xdd\xadYz3\xca\xa9\xc9\x91'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:44:30.605482 (ThreadPoolExecutor-0_0): TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=2, sqlState=None, errorCode=None, errorMessage=None, taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:44:30.605700 (ThreadPoolExecutor-0_0): Poll status: 2, query complete
2021-06-29 13:44:30.605792 (ThreadPoolExecutor-0_0): SQL status: OK in 1.91 seconds
2021-06-29 13:44:30.640885 (ThreadPoolExecutor-0_0): TGetResultSetMetadataResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), schema=TTableSchema(columns=[TColumnDesc(columnName='namespace', typeDesc=TTypeDesc(types=[TTypeEntry(primitiveEntry=TPrimitiveTypeEntry(type=7, typeQualifiers=None), arrayEntry=None, mapEntry=None, structEntry=None, unionEntry=None, userDefinedTypeEntry=None)]), position=1, comment='')]))
2021-06-29 13:44:30.673690 (ThreadPoolExecutor-0_0): TFetchResultsResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), hasMoreRows=False, results=TRowSet(startRowOffset=0, rows=[], columns=[TColumn(boolVal=None, byteVal=None, i16Val=None, i32Val=None, i64Val=None, doubleVal=None, stringVal=TStringColumn(values=['clean', 'default', 'processed', 'raw'], nulls=b'\x00'), binaryVal=None)], binaryColumns=None, columnCount=None))
2021-06-29 13:44:30.674648 (ThreadPoolExecutor-0_0): TFetchResultsResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), hasMoreRows=False, results=TRowSet(startRowOffset=4, rows=[], columns=[TColumn(boolVal=None, byteVal=None, i16Val=None, i32Val=None, i64Val=None, doubleVal=None, stringVal=TStringColumn(values=[], nulls=b'\x00'), binaryVal=None)], binaryColumns=None, columnCount=None))
2021-06-29 13:44:30.677077 (ThreadPoolExecutor-0_0): On list_schemas: Close
2021-06-29 13:44:30.677802 (ThreadPoolExecutor-0_0): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:44:30.678495 (ThreadPoolExecutor-0_0): Acquiring new spark connection "create__nyc_taxi_analytics".
2021-06-29 13:44:30.678635 (ThreadPoolExecutor-0_0): unclosed <socket.socket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.64.25', 51498), raddr=('192.168.64.6', 10000)>
2021-06-29 13:44:30.678856 (ThreadPoolExecutor-0_0): Acquiring new spark connection "create__nyc_taxi_analytics".
2021-06-29 13:44:30.678994 (ThreadPoolExecutor-0_0): Creating schema "nyc_taxi_analytics"
2021-06-29 13:44:30.683761 (ThreadPoolExecutor-0_0): NotImplemented: add_begin_query
2021-06-29 13:44:30.683894 (ThreadPoolExecutor-0_0): Using spark connection "create__nyc_taxi_analytics".
2021-06-29 13:44:30.683966 (ThreadPoolExecutor-0_0): On create__nyc_taxi_analytics: /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "create__nyc_taxi_analytics"} */
create schema if not exists nyc_taxi_analytics
  
2021-06-29 13:44:30.684054 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state closed
2021-06-29 13:44:30.702101 (ThreadPoolExecutor-0_0): TOpenSessionResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), serverProtocolVersion=5, sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\x9dc\x07\xfa\xf3(@g\xbc"\xdaRd\x14\x80\xf4', secret=b'\xc7>T`\x16\xf0D\x1c\xbd3\xda(\xd4\x0b\xce-')), configuration={})
2021-06-29 13:44:30.702364 (ThreadPoolExecutor-0_0): USE `default`
2021-06-29 13:44:30.702489 (ThreadPoolExecutor-0_0): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\x9dc\x07\xfa\xf3(@g\xbc"\xdaRd\x14\x80\xf4', secret=b'\xc7>T`\x16\xf0D\x1c\xbd3\xda(\xd4\x0b\xce-')), statement='USE `default`', confOverlay=None, runAsync=False, queryTimeout=0)
2021-06-29 13:44:30.724037 (ThreadPoolExecutor-0_0): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\x87\xabs\xbf`\xaaB\xac\x9c!\tiwCr\xda', secret=b'\x9ePb\x97\xce\xe9@\xb4\xab\x97\xfebR\xe7\xf5\x96'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:44:30.725300 (ThreadPoolExecutor-0_0): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:44:30.725540 (ThreadPoolExecutor-0_0): /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "create__nyc_taxi_analytics"} */
create schema if not exists nyc_taxi_analytics
  
2021-06-29 13:44:30.725657 (ThreadPoolExecutor-0_0): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\x9dc\x07\xfa\xf3(@g\xbc"\xdaRd\x14\x80\xf4', secret=b'\xc7>T`\x16\xf0D\x1c\xbd3\xda(\xd4\x0b\xce-')), statement='/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "create__nyc_taxi_analytics"} */\ncreate schema if not exists nyc_taxi_analytics\n  ', confOverlay=None, runAsync=True, queryTimeout=0)
2021-06-29 13:44:30.729265 (ThreadPoolExecutor-0_0): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\xfd.(\x9bW\x8fE\x10\xb5+\x94B\x87\x11p{', secret=b'\xea\xac\xd9\x99\x93\x9dB\x02\x8c\x14\x99\xc7L!\xdf#'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:44:30.767056 (ThreadPoolExecutor-0_0): TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)\n\tat org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\t... 16 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 47 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:44:30.767353 (ThreadPoolExecutor-0_0): Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)\n\tat org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\t... 16 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 47 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:44:30.767476 (ThreadPoolExecutor-0_0): Poll status: 5
2021-06-29 13:44:30.767577 (ThreadPoolExecutor-0_0): Error while running:
/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "create__nyc_taxi_analytics"} */
create schema if not exists nyc_taxi_analytics
  
2021-06-29 13:44:30.767651 (ThreadPoolExecutor-0_0): Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
  	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
  	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
  	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
  	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
  	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
  	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
  	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
  	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	... 16 more
  Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
  	... 47 more
  
2021-06-29 13:44:30.767889 (ThreadPoolExecutor-0_0): Error while running:
macro create_schema
2021-06-29 13:44:30.767961 (ThreadPoolExecutor-0_0): Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:748)
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
    	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
    	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
    	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
    	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
    	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	... 16 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
    	... 47 more
    
2021-06-29 13:44:30.768102 (ThreadPoolExecutor-0_0): On create__nyc_taxi_analytics: ROLLBACK
2021-06-29 13:44:30.768158 (ThreadPoolExecutor-0_0): NotImplemented: rollback
2021-06-29 13:44:30.768209 (ThreadPoolExecutor-0_0): On create__nyc_taxi_analytics: Close
2021-06-29 13:44:30.769078 (ThreadPoolExecutor-0_0): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:44:30.770106 (MainThread): Connection 'master' was properly closed.
2021-06-29 13:44:30.770296 (MainThread): Connection 'create__nyc_taxi_analytics' was properly closed.
2021-06-29 13:44:30.770525 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4897a32d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4899790dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4897943520>]}
2021-06-29 13:44:30.770781 (MainThread): Flushing usage events
2021-06-29 13:44:31.739414 (MainThread): Encountered an error:
2021-06-29 13:44:31.739995 (MainThread): Runtime Error
  Runtime Error
    Database Error
      org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
      	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
      	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
      	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
      	at java.security.AccessController.doPrivileged(Native Method)
      	at javax.security.auth.Subject.doAs(Subject.java:422)
      	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
      	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
      	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
      	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
      	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
      	at java.lang.Thread.run(Thread.java:748)
      Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
      	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
      	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
      	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
      	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
      	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
      	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
      	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
      	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
      	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
      	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
      	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
      	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
      	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
      	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
      	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
      	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
      	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
      	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
      	... 16 more
      Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
      	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
      	... 47 more
      
2021-06-29 13:44:31.743357 (MainThread): Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 263, in exception_handler
    yield
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 80, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 208, in execute
    dbt.exceptions.raise_database_error(poll_state.errorMessage)
  File "/usr/local/lib/python3.8/site-packages/dbt/exceptions.py", line 439, in raise_database_error
    raise DatabaseException(msg, node)
dbt.exceptions.DatabaseException: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
  	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
  	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
  	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
  	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
  	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
  	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
  	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
  	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	... 16 more
  Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
  	... 47 more
  

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 263, in exception_handler
    yield
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 1002, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 332, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 259, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 19, in macro
  File "/usr/local/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 332, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 259, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 28, in macro
  File "/usr/local/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 332, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 259, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 227, in execute
    return self.connections.execute(
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 124, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 87, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 276, in exception_handler
    raise dbt.exceptions.RuntimeException(str(exc))
dbt.exceptions.RuntimeException: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:748)
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
    	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
    	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
    	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
    	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
    	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	... 16 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
    	... 47 more
    

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/main.py", line 125, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/lib/python3.8/site-packages/dbt/main.py", line 203, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/lib/python3.8/site-packages/dbt/main.py", line 256, in run_from_args
    results = task.run()
  File "/usr/local/lib/python3.8/site-packages/dbt/task/runnable.py", line 426, in run
    result = self.execute_with_hooks(selected_uids)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/runnable.py", line 383, in execute_with_hooks
    self.before_run(adapter, selected_uids)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/run.py", line 409, in before_run
    self.create_schemas(adapter, selected_uids)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/runnable.py", line 538, in create_schemas
    create_future.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.8/concurrent/futures/thread.py", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/utils.py", line 469, in connected
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/runnable.py", line 500, in create_schema
    adapter.create_schema(relation)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/impl.py", line 182, in create_schema
    self.execute_macro(CREATE_SCHEMA_MACRO_NAME, kwargs=kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 1002, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 276, in exception_handler
    raise dbt.exceptions.RuntimeException(str(exc))
dbt.exceptions.RuntimeException: Runtime Error
  Runtime Error
    Database Error
      org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
      	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
      	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
      	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
      	at java.security.AccessController.doPrivileged(Native Method)
      	at javax.security.auth.Subject.doAs(Subject.java:422)
      	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
      	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
      	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
      	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
      	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
      	at java.lang.Thread.run(Thread.java:748)
      Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
      	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
      	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
      	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
      	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
      	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
      	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
      	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
      	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
      	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
      	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
      	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
      	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
      	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
      	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
      	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
      	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
      	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
      	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
      	... 16 more
      Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
      	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
      	... 47 more
      

2021-06-29 13:55:58.592650 (MainThread): Running with dbt=0.19.1
2021-06-29 13:55:58.604857 (MainThread): the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
2021-06-29 13:55:58.658740 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/root/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2021-06-29 13:55:58.659226 (MainThread): Tracking: tracking
2021-06-29 13:55:58.660957 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2cffe84880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2cffcb8610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2cffcb8730>]}
2021-06-29 13:55:58.668083 (MainThread): Partial parsing not enabled
2021-06-29 13:55:58.668798 (MainThread): Parsing macros/adapters.sql
2021-06-29 13:55:58.687953 (MainThread): Parsing macros/materializations/table.sql
2021-06-29 13:55:58.690479 (MainThread): Parsing macros/materializations/snapshot.sql
2021-06-29 13:55:58.710778 (MainThread): Parsing macros/materializations/view.sql
2021-06-29 13:55:58.711282 (MainThread): Parsing macros/materializations/seed.sql
2021-06-29 13:55:58.723068 (MainThread): Parsing macros/materializations/incremental/strategies.sql
2021-06-29 13:55:58.727890 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-06-29 13:55:58.732442 (MainThread): Parsing macros/materializations/incremental/validate.sql
2021-06-29 13:55:58.737700 (MainThread): Parsing macros/core.sql
2021-06-29 13:55:58.741000 (MainThread): Parsing macros/adapters/common.sql
2021-06-29 13:55:58.776219 (MainThread): Parsing macros/schema_tests/unique.sql
2021-06-29 13:55:58.777686 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2021-06-29 13:55:58.779858 (MainThread): Parsing macros/schema_tests/not_null.sql
2021-06-29 13:55:58.781074 (MainThread): Parsing macros/schema_tests/relationships.sql
2021-06-29 13:55:58.782565 (MainThread): Parsing macros/etc/datetime.sql
2021-06-29 13:55:58.789692 (MainThread): Parsing macros/etc/query.sql
2021-06-29 13:55:58.790520 (MainThread): Parsing macros/etc/is_incremental.sql
2021-06-29 13:55:58.791781 (MainThread): Parsing macros/etc/get_custom_database.sql
2021-06-29 13:55:58.793084 (MainThread): Parsing macros/etc/get_custom_alias.sql
2021-06-29 13:55:58.793796 (MainThread): Parsing macros/etc/get_custom_schema.sql
2021-06-29 13:55:58.795328 (MainThread): Parsing macros/materializations/helpers.sql
2021-06-29 13:55:58.802320 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2021-06-29 13:55:58.816635 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2021-06-29 13:55:58.840531 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2021-06-29 13:55:58.841915 (MainThread): Parsing macros/materializations/table/table.sql
2021-06-29 13:55:58.847303 (MainThread): Parsing macros/materializations/common/merge.sql
2021-06-29 13:55:58.858366 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2021-06-29 13:55:58.859945 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2021-06-29 13:55:58.892531 (MainThread): Parsing macros/materializations/seed/seed.sql
2021-06-29 13:55:58.909309 (MainThread): Parsing macros/materializations/view/view.sql
2021-06-29 13:55:58.914365 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2021-06-29 13:55:58.922263 (MainThread): Partial parsing not enabled
2021-06-29 13:55:58.941371 (MainThread): Acquiring new spark connection "model.nyc_taxi.daily_trips".
2021-06-29 13:55:58.995288 (MainThread): Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '93e5d425-892f-40e0-bb04-6378c6b9dac0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2cffa25850>]}
2021-06-29 13:55:58.998717 (MainThread): Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '93e5d425-892f-40e0-bb04-6378c6b9dac0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2cffa8cbb0>]}
2021-06-29 13:55:58.998883 (MainThread): Found 1 model, 3 tests, 0 snapshots, 0 analyses, 158 macros, 0 operations, 0 seed files, 0 sources, 0 exposures
2021-06-29 13:55:58.999351 (MainThread): 
2021-06-29 13:55:58.999566 (MainThread): Acquiring new spark connection "master".
2021-06-29 13:55:59.000513 (ThreadPoolExecutor-0_0): Acquiring new spark connection "list_schemas".
2021-06-29 13:55:59.012809 (ThreadPoolExecutor-0_0): Using spark connection "list_schemas".
2021-06-29 13:55:59.012934 (ThreadPoolExecutor-0_0): On list_schemas: /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
2021-06-29 13:55:59.013006 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2021-06-29 13:55:59.321873 (ThreadPoolExecutor-0_0): TOpenSessionResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), serverProtocolVersion=5, sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\r\x82A\x88\xba\xafB\x06\x91{\xd0sq\xa2\x10\xa0', secret=b'\xff*\n\x1f]\xbaB6\x87\xed9\x80K\xd9F\xf4')), configuration={})
2021-06-29 13:55:59.322291 (ThreadPoolExecutor-0_0): USE `default`
2021-06-29 13:55:59.322383 (ThreadPoolExecutor-0_0): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\r\x82A\x88\xba\xafB\x06\x91{\xd0sq\xa2\x10\xa0', secret=b'\xff*\n\x1f]\xbaB6\x87\xed9\x80K\xd9F\xf4')), statement='USE `default`', confOverlay=None, runAsync=False, queryTimeout=0)
2021-06-29 13:56:00.577925 (ThreadPoolExecutor-0_0): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\x9b?\xe9\x19\xc0\xe3I\xbf\x82\xa0\xb8\xf5\xc9\x0b\x15\x05', secret=b'\xc4\x9f!r\x08IDU\x8b\xaa\xd3\xc7\x8f\x14\x91\t'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:56:00.587581 (ThreadPoolExecutor-0_0): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:56:00.587903 (ThreadPoolExecutor-0_0): /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
2021-06-29 13:56:00.588036 (ThreadPoolExecutor-0_0): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\r\x82A\x88\xba\xafB\x06\x91{\xd0sq\xa2\x10\xa0', secret=b'\xff*\n\x1f]\xbaB6\x87\xed9\x80K\xd9F\xf4')), statement='/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "list_schemas"} */\n\n    show databases\n  ', confOverlay=None, runAsync=True, queryTimeout=0)
2021-06-29 13:56:00.591182 (ThreadPoolExecutor-0_0): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\xef\x9b\x1a[\xc6\xbeC\xc3\xb7\r\xee\xef\x186\xc6\xd3', secret=b'\xcf\xa5KuXTJ\x91\xa5\x95\xb83\xcc\x8b\xc7"'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:56:00.864188 (ThreadPoolExecutor-0_0): TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=2, sqlState=None, errorCode=None, errorMessage=None, taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:56:00.864400 (ThreadPoolExecutor-0_0): Poll status: 2, query complete
2021-06-29 13:56:00.864463 (ThreadPoolExecutor-0_0): SQL status: OK in 1.85 seconds
2021-06-29 13:56:00.897715 (ThreadPoolExecutor-0_0): TGetResultSetMetadataResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), schema=TTableSchema(columns=[TColumnDesc(columnName='namespace', typeDesc=TTypeDesc(types=[TTypeEntry(primitiveEntry=TPrimitiveTypeEntry(type=7, typeQualifiers=None), arrayEntry=None, mapEntry=None, structEntry=None, unionEntry=None, userDefinedTypeEntry=None)]), position=1, comment='')]))
2021-06-29 13:56:00.934563 (ThreadPoolExecutor-0_0): TFetchResultsResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), hasMoreRows=False, results=TRowSet(startRowOffset=0, rows=[], columns=[TColumn(boolVal=None, byteVal=None, i16Val=None, i32Val=None, i64Val=None, doubleVal=None, stringVal=TStringColumn(values=['clean', 'default', 'processed', 'raw'], nulls=b'\x00'), binaryVal=None)], binaryColumns=None, columnCount=None))
2021-06-29 13:56:00.935417 (ThreadPoolExecutor-0_0): TFetchResultsResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), hasMoreRows=False, results=TRowSet(startRowOffset=4, rows=[], columns=[TColumn(boolVal=None, byteVal=None, i16Val=None, i32Val=None, i64Val=None, doubleVal=None, stringVal=TStringColumn(values=[], nulls=b'\x00'), binaryVal=None)], binaryColumns=None, columnCount=None))
2021-06-29 13:56:00.937665 (ThreadPoolExecutor-0_0): On list_schemas: Close
2021-06-29 13:56:00.938300 (ThreadPoolExecutor-0_0): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:56:00.938877 (ThreadPoolExecutor-0_0): Acquiring new spark connection "create__nyc_taxi_analytics".
2021-06-29 13:56:00.939009 (ThreadPoolExecutor-0_0): unclosed <socket.socket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.64.25', 52362), raddr=('192.168.64.6', 10000)>
2021-06-29 13:56:00.939220 (ThreadPoolExecutor-0_0): Acquiring new spark connection "create__nyc_taxi_analytics".
2021-06-29 13:56:00.939359 (ThreadPoolExecutor-0_0): Creating schema "nyc_taxi_analytics"
2021-06-29 13:56:00.943533 (ThreadPoolExecutor-0_0): NotImplemented: add_begin_query
2021-06-29 13:56:00.943612 (ThreadPoolExecutor-0_0): Using spark connection "create__nyc_taxi_analytics".
2021-06-29 13:56:00.943670 (ThreadPoolExecutor-0_0): On create__nyc_taxi_analytics: /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "create__nyc_taxi_analytics"} */
create schema if not exists nyc_taxi_analytics
  
2021-06-29 13:56:00.943736 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state closed
2021-06-29 13:56:00.959780 (ThreadPoolExecutor-0_0): TOpenSessionResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), serverProtocolVersion=5, sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\xda\xfesx~\xc4J\xa9\x93w\x89\xc9\xa2\t(6', secret=b'\x8b?*\x87\xe2\xa5K\xf0\xa9-\x99\xf8s\xef\xfd\xe5')), configuration={})
2021-06-29 13:56:00.960011 (ThreadPoolExecutor-0_0): USE `default`
2021-06-29 13:56:00.960098 (ThreadPoolExecutor-0_0): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\xda\xfesx~\xc4J\xa9\x93w\x89\xc9\xa2\t(6', secret=b'\x8b?*\x87\xe2\xa5K\xf0\xa9-\x99\xf8s\xef\xfd\xe5')), statement='USE `default`', confOverlay=None, runAsync=False, queryTimeout=0)
2021-06-29 13:56:00.984355 (ThreadPoolExecutor-0_0): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\x8a\xe4C?\x16\xc6N\x03\xa0k=}#\xe4/\x80', secret=b'\x15^\x85\xd0^7C\xd9\xa9N\t\xdb\x1b\xbf\xcd\x91'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:56:00.985842 (ThreadPoolExecutor-0_0): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:56:00.986061 (ThreadPoolExecutor-0_0): /* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "create__nyc_taxi_analytics"} */
create schema if not exists nyc_taxi_analytics
  
2021-06-29 13:56:00.986185 (ThreadPoolExecutor-0_0): TExecuteStatementReq(sessionHandle=TSessionHandle(sessionId=THandleIdentifier(guid=b'\xda\xfesx~\xc4J\xa9\x93w\x89\xc9\xa2\t(6', secret=b'\x8b?*\x87\xe2\xa5K\xf0\xa9-\x99\xf8s\xef\xfd\xe5')), statement='/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "create__nyc_taxi_analytics"} */\ncreate schema if not exists nyc_taxi_analytics\n  ', confOverlay=None, runAsync=True, queryTimeout=0)
2021-06-29 13:56:00.989009 (ThreadPoolExecutor-0_0): TExecuteStatementResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationHandle=TOperationHandle(operationId=THandleIdentifier(guid=b'\xbc\xaa\xcd\xe4L"G\xfb\x99\x86\x03\n\x89\x03\x8c\xd5', secret=b'\x05\xaa\xa8\xcc\xd2\x02D*\xb2\xb7>\x8e\xac\x01\xbf\x9e'), operationType=0, hasResultSet=True, modifiedRowCount=None))
2021-06-29 13:56:01.025005 (ThreadPoolExecutor-0_0): TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)\n\tat org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\t... 16 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 47 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:56:01.025262 (ThreadPoolExecutor-0_0): Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState=None, errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)\n\tat org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n\t... 16 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 47 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
2021-06-29 13:56:01.025362 (ThreadPoolExecutor-0_0): Poll status: 5
2021-06-29 13:56:01.025459 (ThreadPoolExecutor-0_0): Error while running:
/* {"app": "dbt", "dbt_version": "0.19.1", "profile_name": "taxi_db", "target_name": "dev", "connection_name": "create__nyc_taxi_analytics"} */
create schema if not exists nyc_taxi_analytics
  
2021-06-29 13:56:01.025525 (ThreadPoolExecutor-0_0): Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
  	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
  	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
  	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
  	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
  	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
  	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
  	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
  	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	... 16 more
  Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
  	... 47 more
  
2021-06-29 13:56:01.025731 (ThreadPoolExecutor-0_0): Error while running:
macro create_schema
2021-06-29 13:56:01.025801 (ThreadPoolExecutor-0_0): Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:748)
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
    	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
    	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
    	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
    	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
    	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	... 16 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
    	... 47 more
    
2021-06-29 13:56:01.025967 (ThreadPoolExecutor-0_0): On create__nyc_taxi_analytics: ROLLBACK
2021-06-29 13:56:01.026037 (ThreadPoolExecutor-0_0): NotImplemented: rollback
2021-06-29 13:56:01.026101 (ThreadPoolExecutor-0_0): On create__nyc_taxi_analytics: Close
2021-06-29 13:56:01.026806 (ThreadPoolExecutor-0_0): TCloseOperationResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None))
2021-06-29 13:56:01.027837 (MainThread): Connection 'master' was properly closed.
2021-06-29 13:56:01.028028 (MainThread): Connection 'create__nyc_taxi_analytics' was properly closed.
2021-06-29 13:56:01.028254 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2cffa5b370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2cffb11d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2cffa22670>]}
2021-06-29 13:56:01.028509 (MainThread): Flushing usage events
2021-06-29 13:56:02.028796 (MainThread): Encountered an error:
2021-06-29 13:56:02.029200 (MainThread): Runtime Error
  Runtime Error
    Database Error
      org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
      	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
      	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
      	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
      	at java.security.AccessController.doPrivileged(Native Method)
      	at javax.security.auth.Subject.doAs(Subject.java:422)
      	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
      	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
      	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
      	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
      	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
      	at java.lang.Thread.run(Thread.java:748)
      Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
      	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
      	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
      	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
      	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
      	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
      	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
      	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
      	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
      	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
      	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
      	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
      	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
      	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
      	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
      	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
      	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
      	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
      	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
      	... 16 more
      Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
      	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
      	... 47 more
      
2021-06-29 13:56:02.031374 (MainThread): Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 263, in exception_handler
    yield
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 80, in add_query
    cursor.execute(sql, bindings)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 208, in execute
    dbt.exceptions.raise_database_error(poll_state.errorMessage)
  File "/usr/local/lib/python3.8/site-packages/dbt/exceptions.py", line 439, in raise_database_error
    raise DatabaseException(msg, node)
dbt.exceptions.DatabaseException: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
  	at java.security.AccessController.doPrivileged(Native Method)
  	at javax.security.auth.Subject.doAs(Subject.java:422)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
  	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:748)
  Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
  	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
  	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
  	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
  	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
  	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
  	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
  	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
  	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
  	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
  	... 16 more
  Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
  	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
  	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
  	... 47 more
  

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 263, in exception_handler
    yield
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 1002, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 332, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 259, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 19, in macro
  File "/usr/local/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 332, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 259, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 28, in macro
  File "/usr/local/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 332, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/clients/jinja.py", line 259, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 675, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 679, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 41, in macro
  File "/usr/local/lib/python3.8/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/jinja2/runtime.py", line 290, in call
    return __obj(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 227, in execute
    return self.connections.execute(
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 124, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/connections.py", line 87, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 276, in exception_handler
    raise dbt.exceptions.RuntimeException(str(exc))
dbt.exceptions.RuntimeException: Runtime Error
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:422)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    	at java.lang.Thread.run(Thread.java:748)
    Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
    	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
    	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
    	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
    	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
    	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
    	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
    	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
    	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
    	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
    	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
    	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
    	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
    	... 16 more
    Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
    	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
    	... 47 more
    

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/dbt/main.py", line 125, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/lib/python3.8/site-packages/dbt/main.py", line 203, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/lib/python3.8/site-packages/dbt/main.py", line 256, in run_from_args
    results = task.run()
  File "/usr/local/lib/python3.8/site-packages/dbt/task/runnable.py", line 426, in run
    result = self.execute_with_hooks(selected_uids)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/runnable.py", line 383, in execute_with_hooks
    self.before_run(adapter, selected_uids)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/run.py", line 409, in before_run
    self.create_schemas(adapter, selected_uids)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/runnable.py", line 538, in create_schemas
    create_future.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.8/concurrent/futures/thread.py", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/utils.py", line 469, in connected
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/task/runnable.py", line 500, in create_schema
    adapter.create_schema(relation)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/sql/impl.py", line 182, in create_schema
    self.execute_macro(CREATE_SCHEMA_MACRO_NAME, kwargs=kwargs)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/base/impl.py", line 1002, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/lib/python3.8/contextlib.py", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.8/site-packages/dbt/adapters/spark/connections.py", line 276, in exception_handler
    raise dbt.exceptions.RuntimeException(str(exc))
dbt.exceptions.RuntimeException: Runtime Error
  Runtime Error
    Database Error
      org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)
      	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
      	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
      	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)
      	at java.security.AccessController.doPrivileged(Native Method)
      	at javax.security.auth.Subject.doAs(Subject.java:422)
      	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)
      	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
      	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
      	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
      	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
      	at java.lang.Thread.run(Thread.java:748)
      Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)
      	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)
      	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
      	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
      	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
      	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
      	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
      	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:186)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedDBPath(SessionCatalog.scala:234)
      	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:225)
      	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:82)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
      	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
      	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
      	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
      	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
      	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
      	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
      	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
      	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
      	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
      	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
      	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
      	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
      	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:650)
      	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)
      	... 16 more
      Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
      	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)
      	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)
      	... 47 more
      

